[33mcommit 1cb4940c0140c5333d662c5a6468540b49b888eb[m[33m ([m[1;36mHEAD -> [m[1;32mmain[m[33m)[m
Author: Miranda Eriksson Thultrup <Miranda.thulstrup@gmail.com>
Date:   Wed May 22 20:47:42 2024 +0200

    added read_me file

[1mdiff --git a/Uppgift_2/.py b/Uppgift_2/.py[m
[1mnew file mode 100644[m
[1mindex 0000000..87a05b5[m
[1m--- /dev/null[m
[1m+++ b/Uppgift_2/.py[m
[36m@@ -0,0 +1,25 @@[m
[32m+[m[32mimport torch[m[41m[m
[32m+[m[32mimport torch.nn as nn[m[41m[m
[32m+[m[32mimport torch.nn.functional as F[m[41m[m
[32m+[m[41m[m
[32m+[m[32mclass AnimalClassifier(nn.Module):[m[41m[m
[32m+[m[32m    def __init__(self, num_classes):[m[41m[m
[32m+[m[32m        super(AnimalClassifier, self).__init__()[m[41m[m
[32m+[m[32m        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # First convolutional layer[m[41m[m
[32m+[m[32m        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # Second convolutional layer[m[41m[m
[32m+[m[32m        self.pool = nn.MaxPool2d(2, 2)  # Pooling layer to reduce dimensions[m[41m[m
[32m+[m[32m        self.fc1 = nn.Linear(64 * 32 * 32, 256)  # Fully connected layer[m[41m[m
[32m+[m[32m        self.fc2 = nn.Linear(256, num_classes)  # Output layer with num_classes[m[41m[m
[32m+[m[41m        [m
[32m+[m[32m    def forward(self, x):[m[41m[m
[32m+[m[32m        # Forward pass through the model[m[41m[m
[32m+[m[32m        x = F.relu(self.conv1(x))  # ReLU activation for the first convolution[m[41m[m
[32m+[m[32m        x = self.pool(x)  # Apply pooling[m[41m[m
[32m+[m[32m        x = F.relu(self.conv2(x))  # ReLU activation for the second convolution[m[41m[m
[32m+[m[32m        x = self.pool(x)  # Apply pooling again[m[41m[m
[32m+[m[32m        x = x.view(-1, 64 * 32 * 32)  # Flatten the tensor for the fully connected layer[m[41m[m
[32m+[m[32m        x = F.relu(self.fc1(x))  # ReLU activation for the first fully connected layer[m[41m[m
[32m+[m[32m        x = self.fc2(x)  # Output layer with no activation (suitable for classification)[m[41m[m
[32m+[m[32m        return x[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[1mdiff --git a/Uppgift_2/B.py b/Uppgift_2/B.py[m
[1mnew file mode 100644[m
[1mindex 0000000..ee0ceff[m
[1m--- /dev/null[m
[1m+++ b/Uppgift_2/B.py[m
[36m@@ -0,0 +1,42 @@[m
[32m+[m[32mimport numpy as np[m[41m[m
[32m+[m[41m[m
[32m+[m[32mdef sigmoid(x):[m[41m[m
[32m+[m[32m    y = 1 / (1 + np.exp(-x))[m[41m [m
[32m+[m[32m    return y[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[32m+[m[32mclass Perceptron():[m[41m[m
[32m+[m[32m    def __init__(self, architechture):[m[41m[m
[32m+[m[32m        weights_values = {}[m[41m[m
[32m+[m[32m        bias_values = {}[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        for i, layer in enumerate(architechture):[m[41m[m
[32m+[m[32m            input_size = layer["input_size"][m[41m[m
[32m+[m[32m            output_size = layer["output_size"][m[41m[m
[32m+[m[41m[m
[32m+[m[32m            weights_values["w" + "_" +[m[41m[m
[32m+[m[32m                          str(i)] = np.random.randn(output_size, input_size) * 0.1[m[41m[m
[32m+[m[32m            bias_values["b" + "_" +[m[41m[m
[32m+[m[32m                        str(i)] = np.random.randn(output_size, 1) * 0.1[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        self.weights = weights_values[m[41m[m
[32m+[m[32m        self.bias = bias_values[m[41m[m
[32m+[m[41m[m
[32m+[m[32m    def forward(self, x):[m[41m[m
[32m+[m[32m        for i in range(len(self.weights)):[m[41m[m
[32m+[m[32m            x = np.dot(self.weights["w_" + str(i)], x) + self.bias["b_" + str(i)][m[41m[m
[32m+[m[32m            x = sigmoid(x)[m[41m[m
[32m+[m[32m        return x[m[41m[m
[32m+[m[41m        [m
[32m+[m[41m[m
[32m+[m[32marchitecture = [[m[41m[m
[32m+[m[32m    {"input_size": 3, "output_size": 5},[m[41m  [m
[32m+[m[32m    {"input_size": 5, "output_size": 2}[m[41m   [m
[32m+[m[32m][m[41m[m
[32m+[m[41m[m
[32m+[m[32mparameters = Perceptron(architecture)[m[41m[m
[32m+[m[41m[m
[32m+[m[32mx = inputs = np.array([1, 0.2, 0.9])[m[41m[m
[32m+[m[41m[m
[32m+[m[32mprint(parameters.forward(x))[m
\ No newline at end of file[m
[1mdiff --git a/Uppgift_2/C.py b/Uppgift_2/C.py[m
[1mnew file mode 100644[m
[1mindex 0000000..c1d73e6[m
[1m--- /dev/null[m
[1m+++ b/Uppgift_2/C.py[m
[36m@@ -0,0 +1,162 @@[m
[32m+[m[32mimport torch[m[41m[m
[32m+[m[32mimport torch.nn as nn[m[41m[m
[32m+[m[32mimport torch.optim as optim[m[41m[m
[32m+[m[32mfrom torchvision import datasets, transforms[m[41m[m
[32m+[m[32mfrom torch.utils.data import DataLoader[m[41m[m
[32m+[m[32mfrom torch.cuda.amp import autocast, GradScaler[m[41m[m
[32m+[m[32mfrom torch.optim.lr_scheduler import ReduceLROnPlateau[m[41m[m
[32m+[m[32mimport matplotlib.pyplot as plt[m[41m[m
[32m+[m[32mimport numpy as np[m[41m[m
[32m+[m[32mimport time[m[41m[m
[32m+[m[41m[m
[32m+[m[32mclass Perceptron(nn.Module):[m[41m[m
[32m+[m[32m    def __init__(self, layers):[m[41m[m
[32m+[m[32m        super(Perceptron, self).__init__()[m[41m[m
[32m+[m[32m        self.flatten = nn.Flatten()[m[41m[m
[32m+[m[32m        self.layers = nn.ModuleList()[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        for i, layer in enumerate(layers):[m[41m[m
[32m+[m[32m            self.layers.append(nn.Linear(layer["input_size"], layer["output_size"]))[m[41m[m
[32m+[m[32m            if i < len(layers) - 1:[m[41m  [m
[32m+[m[32m                self.layers.append(nn.ReLU())[m[41m [m
[32m+[m[41m[m
[32m+[m[32m    def forward(self, x):[m[41m[m
[32m+[m[32m        x = self.flatten(x)[m[41m  [m
[32m+[m[32m        for layer in self.layers:[m[41m[m
[32m+[m[32m            x = layer(x)[m[41m[m
[32m+[m[32m        return x[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[32m+[m[32mdef plot_samples(samples, title):[m[41m[m
[32m+[m[32m    plt.figure(figsize=(10, 5))[m[41m[m
[32m+[m[32m    for i, (image, label, pred) in enumerate(samples):[m[41m[m
[32m+[m[32m        image = image.cpu().numpy().transpose((1, 2, 0))  # Rearrange dimensions for matplotlib[m[41m[m
[32m+[m[32m        mean = np.array([0.5])[m[41m[m
[32m+[m[32m        std = np.array([0.5])[m[41m[m
[32m+[m[32m        image = std * image + mean  # Unnormalize[m[41m[m
[32m+[m[32m        image = np.clip(image, 0, 1)[m[41m[m
[32m+[m[32m        plt.subplot(2, 5, i + 1)[m[41m[m
[32m+[m[32m        plt.imshow(image[:, :, 0], cmap='gray')[m[41m[m
[32m+[m[32m        plt.title(f'Label: {label}\nPred: {pred}')[m[41m[m
[32m+[m[32m        plt.axis('off')[m[41m[m
[32m+[m[32m    plt.suptitle(title)[m[41m[m
[32m+[m[41m[m
[32m+[m[32mdevice = ([m[41m[m
[32m+[m[32m    "cuda" if torch.cuda.is_available() else[m[41m[m
[32m+[m[32m    "mps" if torch.backends.mps.is_available() else[m[41m[m
[32m+[m[32m    "cpu"[m[41m[m
[32m+[m[32m)[m[41m[m
[32m+[m[41m[m
[32m+[m[32mtransform = transforms.Compose([[m[41m [m
[32m+[m[32m    transforms.ToTensor(),[m[41m   [m
[32m+[m[32m    transforms.Normalize((0.5,), (0.5,))[m[41m [m
[32m+[m[32m])[m[41m[m
[32m+[m[41m[m
[32m+[m[32mtrain_set = datasets.MNIST(root='./data', train=True,  download=True, transform=transform)[m[41m[m
[32m+[m[32mtest_set  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)[m[41m[m
[32m+[m[41m[m
[32m+[m[32mtrain_loader = DataLoader(train_set, batch_size=64, shuffle=True, pin_memory=True)[m[41m[m
[32m+[m[32mtest_loader  = DataLoader(test_set,  batch_size=64, shuffle=False)[m[41m[m
[32m+[m[41m[m
[32m+[m[32mlayers = [{"input_size": 784, "output_size": 64},[m[41m[m
[32m+[m[32m          {"input_size": 64, "output_size": 64},[m[41m[m
[32m+[m[32m          {"input_size": 64, "output_size": 32},[m[41m[m
[32m+[m[32m          {"input_size": 32, "output_size": 10}][m[41m[m
[32m+[m[32mmodel = Perceptron(layers=layers)[m[41m[m
[32m+[m[32mmodel = model.to(device)[m[41m[m
[32m+[m[41m[m
[32m+[m[32mcriterion = nn.CrossEntropyLoss()[m[41m[m
[32m+[m[32moptimizer = optim.Adam(model.parameters(), lr=0.01)[m[41m[m
[32m+[m[32mscaler = GradScaler()[m[41m[m
[32m+[m[32mscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)[m[41m[m
[32m+[m[32mtrain_losses, test_losses, train_accuracies, test_accuracies = [], [], [], [][m[41m[m
[32m+[m[32mcorrect_samples = [][m[41m[m
[32m+[m[32mincorrect_samples = [][m[41m[m
[32m+[m[32mtimestamp = time.strftime("%Y%m%d-%H%M%S")[m[41m[m
[32m+[m[32msave_path = "saved_models/model_"+timestamp+"/my_model_epoch_{}.pth"[m[41m[m
[32m+[m[41m[m
[32m+[m[32mnum_epochs = 100[m[41m[m
[32m+[m[32mfor epoch in range(num_epochs):[m[41m[m
[32m+[m[32m    total_correct = 0[m[41m[m
[32m+[m[32m    total_samples = 0[m[41m[m
[32m+[m[41m[m
[32m+[m[32m    for images, labels in train_loader:[m[41m[m
[32m+[m[32m        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        optimizer.zero_grad()[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        with autocast():[m[41m[m
[32m+[m[32m            outputs = model(images)[m[41m[m
[32m+[m[32m            loss = criterion(outputs, labels)[m[41m[m
[32m+[m[32m            _, predicted = torch.max(outputs, 1)  # Get the indices of max logit which are the predicted classes[m[41m[m
[32m+[m[32m            total_correct += (predicted == labels).sum().item()[m[41m[m
[32m+[m[32m            total_samples += labels.size(0)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        scaler.scale(loss).backward()[m[41m[m
[32m+[m[32m        scaler.step(optimizer)[m[41m[m
[32m+[m[32m        scaler.update()[m[41m[m
[32m+[m[41m[m
[32m+[m[32m    train_accuracy = 100 * total_correct / total_samples[m[41m[m
[32m+[m[32m    train_losses.append(loss.item())[m[41m[m
[32m+[m[32m    train_accuracies.append(train_accuracy)[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[32m+[m[32m    model.eval()[m[41m[m
[32m+[m[32m    total_test_correct = 0[m[41m[m
[32m+[m[32m    total_test_samples = 0[m[41m[m
[32m+[m[32m    with torch.no_grad():[m[41m[m
[32m+[m[32m        for images, labels in test_loader:[m[41m[m
[32m+[m[32m            images, labels = images.to(device), labels.to(device)[m[41m[m
[32m+[m[32m            with autocast():[m[41m[m
[32m+[m[32m                outputs = model(images)[m[41m[m
[32m+[m[32m                test_loss = criterion(outputs, labels)[m[41m[m
[32m+[m[32m                _, predicted = torch.max(outputs, 1)[m[41m[m
[32m+[m[32m                total_test_correct += (predicted == labels).sum().item()[m[41m[m
[32m+[m[32m                total_test_samples += labels.size(0)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m                matches = predicted == labels[m[41m[m
[32m+[m[32m                for i in range(images.size(0)):[m[41m[m
[32m+[m[32m                    if len(correct_samples) < 10 and matches[i]:[m[41m [m
[32m+[m[32m                        correct_samples.append((images[i], labels[i], predicted[i]))[m[41m[m
[32m+[m[32m                    elif len(incorrect_samples) < 10 and not matches[i]:[m[41m[m
[32m+[m[32m                        incorrect_samples.append((images[i], labels[i], predicted[i]))[m[41m[m
[32m+[m[41m[m
[32m+[m[32m    test_accuracy = 100 * total_test_correct / total_test_samples[m[41m[m
[32m+[m[32m    test_losses.append(test_loss.item())[m[41m[m
[32m+[m[32m    test_accuracies.append(test_accuracy)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m    scheduler.step(loss)[m[41m[m
[32m+[m[41m    [m
[32m+[m[32m    if (epoch + 1) % 1 == 0:[m[41m[m
[32m+[m[32m        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {loss.item():.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Loss: {test_loss.item():.4f}, Test Accuracy: {test_accuracy:.2f}%')[m[41m[m
[32m+[m[32m    if (epoch + 1) % 10 == 0:[m[41m[m
[32m+[m[32m        torch.save(model.state_dict(), save_path.format(epoch + 1))[m[41m[m
[32m+[m[32m        print(f'Saved model at epoch {epoch + 1}')[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[32m+[m[32m# Plotting training and testing loss[m[41m[m
[32m+[m[32mplt.subplot(1, 2, 1)[m[41m[m
[32m+[m[32mplt.plot(train_losses, label='Train Loss')[m[41m[m
[32m+[m[32mplt.plot(test_losses, label='Test Loss')[m[41m[m
[32m+[m[32mplt.title('Loss over epochs')[m[41m[m
[32m+[m[32mplt.xlabel('Epoch')[m[41m[m
[32m+[m[32mplt.ylabel('Loss')[m[41m[m
[32m+[m[32mplt.legend()[m[41m[m
[32m+[m[41m[m
[32m+[m[32m# Plotting training and testing accuracy[m[41m[m
[32m+[m[32mplt.subplot(1, 2, 2)[m[41m[m
[32m+[m[32mplt.plot(train_accuracies, label='Train Accuracy')[m[41m[m
[32m+[m[32mplt.plot(test_accuracies, label='Test Accuracy')[m[41m[m
[32m+[m[32mplt.title('Accuracy over epochs')[m[41m[m
[32m+[m[32mplt.xlabel('Epoch')[m[41m[m
[32m+[m[32mplt.ylabel('Accuracy (%)')[m[41m[m
[32m+[m[32mplt.legend()[m[41m[m
[32m+[m[41m[m
[32m+[m[32mplot_samples(correct_samples, "Correctly Predicted Samples")[m[41m[m
[32m+[m[32mplot_samples(incorrect_samples, "Incorrectly Predicted Samples")[m[41m[m
[32m+[m[41m[m
[32m+[m[32mplt.tight_layout()[m[41m[m
[32m+[m[32mplt.show()[m
\ No newline at end of file[m
[1mdiff --git a/Uppgift_2/CUB-200.py b/Uppgift_2/CUB-200.py[m
[1mnew file mode 100644[m
[1mindex 0000000..5bd011a[m
[1m--- /dev/null[m
[1m+++ b/Uppgift_2/CUB-200.py[m
[36m@@ -0,0 +1,424 @@[m
[32m+[m[32mimport torch[m[41m[m
[32m+[m[32mimport torch.nn as nn[m[41m[m
[32m+[m[32mimport torch.optim as optim[m[41m[m
[32m+[m[32mimport torch.nn.functional as F[m[41m[m
[32m+[m[32mfrom torch.utils.data import DataLoader[m[41m[m
[32m+[m[32mfrom torch.utils.data import Dataset[m[41m[m
[32m+[m[32mimport os[m[41m[m
[32m+[m[32mimport pandas as pd[m[41m[m
[32m+[m[32mfrom torchvision.io import read_image[m[41m[m
[32m+[m[32mfrom torchvision import transforms[m[41m[m
[32m+[m[32mfrom torch.cuda.amp import autocast, GradScaler[m[41m[m
[32m+[m[32mfrom torch.optim.lr_scheduler import ReduceLROnPlateau[m[41m[m
[32m+[m[32mimport matplotlib.pyplot as plt[m[41m[m
[32m+[m[32mimport numpy as np[m[41m[m
[32m+[m[32mimport time[m[41m[m
[32m+[m[32mimport os[m[41m[m
[32m+[m[32mfrom sklearn.metrics import confusion_matrix, classification_report[m[41m[m
[32m+[m[32mfrom pathlib import Path[m[41m[m
[32m+[m[32mfrom torchvision.transforms import v2[m[41m[m
[32m+[m[32mfrom torchvision.io import read_image[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[32m+[m[32mclass FlexiblePerceptron(nn.Module):[m[41m[m
[32m+[m[32m    def __init__(self, architecture, input_shape, output_size):[m[41m[m
[32m+[m[32m        """[m[41m[m
[32m+[m[32m        Initialize the Perceptron with a given architecture.[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        :param architecture: A list of dictionaries, each representing a layer with its type and parameters.[m[41m[m
[32m+[m[32m        :param input_shape: A tuple representing the input shape (like (channels, height, width)).[m[41m[m
[32m+[m[32m        :param output_size: The output size from the network.[m[41m[m
[32m+[m[32m        """[m[41m[m
[32m+[m[32m        super(FlexiblePerceptron, self).__init__()[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        # Initialize layers list[m[41m[m
[32m+[m[32m        layers = [][m[41m[m
[32m+[m[41m[m
[32m+[m[32m        # Keep track of the current input shape (starts with the given input shape)[m[41m[m
[32m+[m[32m        current_shape = input_shape[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        # Build the architecture[m[41m[m
[32m+[m[32m        for layer_spec in architecture:[m[41m[m
[32m+[m[32m            layer_type = layer_spec["type"][m[41m[m
[32m+[m[32m            layer_params = layer_spec.get("params", {})[m[41m[m
[32m+[m[41m[m
[32m+[m[32m            if layer_type == "Conv2D":[m[41m[m
[32m+[m[32m                layers.append([m[41m[m
[32m+[m[32m                    nn.Conv2d([m[41m[m
[32m+[m[32m                        in_channels=current_shape[0],[m[41m[m
[32m+[m[32m                        out_channels=layer_params["out_channels"],[m[41m[m
[32m+[m[32m                        kernel_size=layer_params["kernel_size"],[m[41m[m
[32m+[m[32m                        stride=layer_params.get("stride", 1),[m[41m[m
[32m+[m[32m                        padding=layer_params.get("padding", 0),[m[41m[m
[32m+[m[32m                    )[m[41m[m
[32m+[m[32m                )[m[41m[m
[32m+[m[32m                # Update the current shape for the Conv2D layer[m[41m[m
[32m+[m[32m                kernel_size = layer_params["kernel_size"][m[41m[m
[32m+[m[32m                stride = layer_params.get("stride", 1)[m[41m[m
[32m+[m[32m                padding = layer_params.get("padding", 0)[m[41m[m
[32m+[m[32m                new_height = ([m[41m[m
[32m+[m[32m                    current_shape[1] - kernel_size + 2 * padding[m[41m[m
[32m+[m[32m                ) // stride + 1[m[41m[m
[32m+[m[32m                new_width = ([m[41m[m
[32m+[m[32m                    current_shape[2] - kernel_size + 2 * padding[m[41m[m
[32m+[m[32m                ) // stride + 1[m[41m[m
[32m+[m[32m                current_shape = ([m[41m[m
[32m+[m[32m                    layer_params["out_channels"], new_height, new_width)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m            elif layer_type == "MaxPool":[m[41m[m
[32m+[m[32m                # MaxPool layer with kernel size, stride, and padding[m[41m[m
[32m+[m[32m                kernel_size = layer_params["kernel_size"][m[41m[m
[32m+[m[32m                # Default stride = kernel size[m[41m[m
[32m+[m[32m                stride = layer_params.get("stride", kernel_size)[m[41m[m
[32m+[m[32m                padding = layer_params.get("padding", 0)[m[41m[m
[32m+[m[32m                layers.append([m[41m[m
[32m+[m[32m                    nn.MaxPool2d([m[41m[m
[32m+[m[32m                        kernel_size=kernel_size,[m[41m[m
[32m+[m[32m                        stride=stride,[m[41m[m
[32m+[m[32m                        padding=padding,[m[41m[m
[32m+[m[32m                    )[m[41m[m
[32m+[m[32m                )[m[41m[m
[32m+[m[32m                # Update the current shape after MaxPool[m[41m[m
[32m+[m[32m                new_height = ([m[41m[m
[32m+[m[32m                    (current_shape[1] - kernel_size + 2 * padding)[m[41m[m
[32m+[m[32m                ) // stride + 1[m[41m[m
[32m+[m[32m                new_width = ([m[41m[m
[32m+[m[32m                    (current_shape[2] - kernel_size + 2 * padding)[m[41m[m
[32m+[m[32m                ) // stride + 1[m[41m[m
[32m+[m[32m                current_shape = ([m[41m[m
[32m+[m[32m                    current_shape[0], new_height, new_width)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m            elif layer_type == "Linear":[m[41m[m
[32m+[m[32m                if len(current_shape) > 1:[m[41m[m
[32m+[m[32m                    layers.append(nn.Flatten())[m[41m[m
[32m+[m[32m                    current_input_size = current_shape[0] * \[m[41m[m
[32m+[m[32m                        current_shape[1] * current_shape[2][m[41m[m
[32m+[m[32m                else:[m[41m[m
[32m+[m[32m                    current_input_size = current_shape[0][m[41m[m
[32m+[m[41m[m
[32m+[m[32m                layers.append([m[41m[m
[32m+[m[32m                    nn.Linear(current_input_size, layer_params["output_size"])[m[41m[m
[32m+[m[32m                )[m[41m[m
[32m+[m[32m                current_shape = (layer_params["output_size"],)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m            elif layer_type == "Dropout":[m[41m[m
[32m+[m[32m                layers.append(nn.Dropout(layer_params["p"]))[m[41m[m
[32m+[m[41m[m
[32m+[m[32m            elif layer_type == "ReLU":[m[41m[m
[32m+[m[32m                layers.append(nn.ReLU())[m[41m[m
[32m+[m[41m[m
[32m+[m[32m            elif layer_type == "BatchNorm":[m[41m[m
[32m+[m[32m                if len(current_shape) > 1:[m[41m[m
[32m+[m[32m                    layers.append(nn.BatchNorm2d(current_shape[0]))[m[41m[m
[32m+[m[32m                else:[m[41m[m
[32m+[m[32m                    layers.append(nn.BatchNorm1d(current_shape[0]))[m[41m[m
[32m+[m[41m[m
[32m+[m[32m            else:[m[41m[m
[32m+[m[32m                raise ValueError(f"Unsupported layer type: {layer_type}")[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        # Add the final output linear layer[m[41m[m
[32m+[m[32m        if len(current_shape) > 1:[m[41m[m
[32m+[m[32m            layers.append(nn.Flatten())[m[41m[m
[32m+[m[32m            current_input_size = current_shape[0] * \[m[41m[m
[32m+[m[32m                current_shape[1] * current_shape[2][m[41m[m
[32m+[m[32m        else:[m[41m[m
[32m+[m[32m            current_input_size = current_shape[0][m[41m[m
[32m+[m[41m[m
[32m+[m[32m        layers.append([m[41m[m
[32m+[m[32m            nn.Linear(current_input_size, output_size)[m[41m[m
[32m+[m[32m        )[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        # Convert layers list to a sequential module[m[41m[m
[32m+[m[32m        self.network = nn.Sequential(*layers)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m    def forward(self, x):[m[41m[m
[32m+[m[32m        """[m[41m[m
[32m+[m[32m        Forward propagation through the network.[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        :param x: Input tensor.[m[41m[m
[32m+[m[32m        :return: Output tensor.[m[41m[m
[32m+[m[32m        """[m[41m[m
[32m+[m[32m        return self.network(x)[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[32m+[m[32m# Define a custom dataset class for CUB-200-2011[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[32m+[m[32mclass CUBDataset(Dataset):[m[41m[m
[32m+[m[32m    def __init__(self, data_dir, transform=None):[m[41m[m
[32m+[m[32m        self.data_dir = data_dir[m[41m[m
[32m+[m[32m        self.transform = transform[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        # Load image-to-class mapping with updated delimiter[m[41m[m
[32m+[m[32m        image_class_path = os.path.join(data_dir, 'images.txt')[m[41m[m
[32m+[m[32m        self.image_class_itterate = pd.read_csv([m[41m[m
[32m+[m[32m            image_class_path, sep=r"\s+", engine='python', header=None)[m[41m[m
[32m+[m[32m        # Construct image paths[m[41m[m
[32m+[m[32m        self.image_paths = [os.path.join(data_dir, 'images', f"{row[1]}") for _, row in self.image_class_itterate.iterrows()][m[41m[m
[32m+[m[32m        image_class_labels_path = os.path.join([m[41m[m
[32m+[m[32m            data_dir, 'image_class_labels.txt')[m[41m[m
[32m+[m[32m        self.image_class_labels = pd.read_csv([m[41m[m
[32m+[m[32m            image_class_labels_path, sep=r"\s+", engine='python', header=None)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m    def __len__(self):[m[41m[m
[32m+[m[32m        return len(self.image_class_labels)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m    def __getitem__(self, idx):[m[41m[m
[32m+[m[32m        image_path = self.image_paths[idx][m[41m[m
[32m+[m[41m[m
[32m+[m[32m        # Check if file exists to prevent "File not found" error[m[41m[m
[32m+[m[32m        if not os.path.exists(image_path):[m[41m[m
[32m+[m[32m            raise ValueError(f"File not found: {image_path}")[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        image = read_image(image_path)  # Read the image[m[41m[m
[32m+[m[32m        # Adjust for zero-based index[m[41m[m
[32m+[m[32m        label = int(self.image_class_labels.iloc[idx, 1]) - 1[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        if self.transform:[m[41m[m
[32m+[m[32m            image = self.transform(image)  # Apply transformation[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        return image, label[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[32m+[m[32mclass GrayscaleToRGB:[m[41m[m
[32m+[m[32m    def __call__(self, img):[m[41m[m
[32m+[m[32m        # If the image is grayscale, convert it to RGB by repeating the channel[m[41m[m
[32m+[m[32m        if img.shape[0] == 1:[m[41m[m
[32m+[m[32m            # Repeat the single channel to make it 3-channel (RGB)[m[41m[m
[32m+[m[32m            img = img.repeat(3, 1, 1)[m[41m[m
[32m+[m[32m        return img[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[32m+[m[32mdef plot_samples(samples, title):[m[41m[m
[32m+[m[32m    plt.figure(figsize=(10, 5))[m[41m[m
[32m+[m[32m    for i, (image, label, pred) in enumerate(samples):[m[41m[m
[32m+[m[32m        image = image.cpu().numpy().transpose((1, 2, 0))  # rearrange for matplotlib[m[41m[m
[32m+[m[32m        mean = np.array([0.5])[m[41m[m
[32m+[m[32m        std = np.array([0.5])[m[41m[m
[32m+[m[32m        image = std * image + mean  # unnormalize[m[41m[m
[32m+[m[32m        image = np.clip(image, 0, 1)[m[41m[m
[32m+[m[32m        plt.subplot(2, 5, i + 1)[m[41m[m
[32m+[m[32m        plt.imshow(image[:, :, 0], cmap="gray")[m[41m[m
[32m+[m[32m        plt.title(f"Label: {label}\nPred: {pred}")[m[41m[m
[32m+[m[32m        plt.axis("off")[m[41m[m
[32m+[m[32m    plt.suptitle(title)[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[32m+[m[32marchitecture = [[m[41m[m
[32m+[m[32m    {"type": "Conv2D", "params": {"out_channels": 64,[m[41m[m
[32m+[m[32m                                  "kernel_size": 5, "stride": 1, "padding": 1}},[m[41m[m
[32m+[m[32m    {"type": "Conv2D", "params": {"out_channels": 64,[m[41m[m
[32m+[m[32m                                  "kernel_size": 5, "stride": 1, "padding": 1}},[m[41m[m
[32m+[m[32m    {[m[41m[m
[32m+[m[32m        "type": "MaxPool",[m[41m[m
[32m+[m[32m        "params": {[m[41m[m
[32m+[m[32m            "kernel_size": 5,  # Kernel size for max-pooling[m[41m[m
[32m+[m[32m            "stride": 2,  # Stride of 2 reduces the spatial dimensions by half[m[41m[m
[32m+[m[32m        },[m[41m[m
[32m+[m[32m    },[m[41m[m
[32m+[m[32m    {"type": "Dropout", "params": {"p": 0.5}},[m[41m[m
[32m+[m[32m    {"type": "Linear", "params": {"output_size": 128}},[m[41m[m
[32m+[m[32m    {"type": "Linear", "params": {"output_size": 128}},[m[41m[m
[32m+[m[32m][m[41m[m
[32m+[m[41m[m
[32m+[m[32m# Define data transformations for data augmentation and normalization[m[41m[m
[32m+[m[32mtrain_transform = transforms.Compose([[m[41m[m
[32m+[m[32m    GrayscaleToRGB(),  # Convert grayscale to RGB if needed[m[41m[m
[32m+[m[32m    transforms.ToPILImage(),[m[41m[m
[32m+[m[32m    transforms.Resize(256),[m[41m[m
[32m+[m[32m    transforms.CenterCrop(224),[m[41m[m
[32m+[m[32m    transforms.RandomHorizontalFlip(),[m[41m[m
[32m+[m[32m    transforms.RandomRotation(10),[m[41m[m
[32m+[m[32m    transforms.ToTensor(),[m[41m[m
[32m+[m[32m    transforms.Normalize([0.485, 0.456, 0.406], [[m[41m[m
[32m+[m[32m                         0.229, 0.224, 0.225])  # Normalization for RGB[m[41m[m
[32m+[m[32m])[m[41m[m
[32m+[m[41m[m
[32m+[m[32mtest_transform = transforms.Compose([[m[41m[m
[32m+[m[32m    GrayscaleToRGB(),  # Ensure RGB consistency[m[41m[m
[32m+[m[32m    transforms.ToPILImage(),[m[41m[m
[32m+[m[32m    transforms.Resize(256),[m[41m[m
[32m+[m[32m    transforms.CenterCrop(224),[m[41m[m
[32m+[m[32m    transforms.ToTensor(),[m[41m[m
[32m+[m[32m    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])[m[41m[m
[32m+[m[32m])[m[41m[m
[32m+[m[41m[m
[32m+[m[32m# Define data directories[m[41m[m
[32m+[m[32mdata_dir = "./data/CUB-200/CUB_200_2011/"[m[41m[m
[32m+[m[32mdevice = ("cuda")[m[41m[m
[32m+[m[41m[m
[32m+[m[32msave_dir = "./models/CUB-200"[m[41m[m
[32m+[m[32mif not os.path.exists(save_dir):[m[41m[m
[32m+[m[32m    os.makedirs(save_dir)[m[41m[m
[32m+[m[41m[m
[32m+[m[32mtimestamp = time.strftime("%Y%m%d-%H%M%S")[m[41m[m
[32m+[m[41m[m
[32m+[m[32mweight_dir = f"{save_dir}/model_{timestamp}/weights"[m[41m[m
[32m+[m[32mresults_dir = f"{save_dir}/model_{timestamp}/results"[m[41m[m
[32m+[m[41m[m
[32m+[m[32mif not os.path.exists(weight_dir):[m[41m[m
[32m+[m[32m    os.makedirs(weight_dir)[m[41m[m
[32m+[m[41m[m
[32m+[m[32msave_path = weight_dir+"/epoch_{}.pth"[m[41m[m
[32m+[m[41m[m
[32m+[m[32m# Create custom datasets for training and testing[m[41m[m
[32m+[m[32mtrain_dataset = CUBDataset(data_dir, transform=train_transform)[m[41m[m
[32m+[m[32mtest_dataset = CUBDataset(data_dir, transform=test_transform)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m# Create DataLoaders for batching and shuffling[m[41m[m
[32m+[m[32mtrain_loader = DataLoader(train_dataset, batch_size=64,[m[41m[m
[32m+[m[32m                          shuffle=True, pin_memory=True)[m[41m[m
[32m+[m[32mtest_loader = DataLoader(test_dataset, batch_size=64,[m[41m[m
[32m+[m[32m                         shuffle=False, pin_memory=True)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m# Create the perceptron[m[41m[m
[32m+[m[32m# Example input shape for an image with 3 channels (e.g., CIFAR-10)[m[41m[m
[32m+[m[32minput_shape = (3, 224, 224)[m[41m[m
[32m+[m[32moutput_size = 200  # Example output size (e.g., 10 classes)[m[41m[m
[32m+[m[32mmodel = FlexiblePerceptron(architecture, input_shape, output_size)[m[41m[m
[32m+[m[32mmodel = model.to(device)[m[41m[m
[32m+[m[41m[m
[32m+[m[32moptimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)[m[41m[m
[32m+[m[32mscaler = GradScaler()[m[41m[m
[32m+[m[32mscheduler = ReduceLROnPlateau(optimizer, mode="min", factor=0.2, patience=10)[m[41m[m
[32m+[m[41m[m
[32m+[m[32mtrain_accuracies = [][m[41m[m
[32m+[m[32mtest_accuracies = [][m[41m[m
[32m+[m[32mtrain_losses = [][m[41m[m
[32m+[m[32mtest_losses = [][m[41m[m
[32m+[m[41m[m
[32m+[m[32mmodel.eval()[m[41m[m
[32m+[m[32minitial_predictions = [][m[41m[m
[32m+[m[32minitial_targets = [][m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[32m+[m[32mwith torch.no_grad(): #no training for first test[m[41m[m
[32m+[m[32m    for images, labels in test_loader:[m[41m[m
[32m+[m[32m        images, labels = images.to(device), labels.to(device)[m[41m[m
[32m+[m[41m        [m
[32m+[m[32m        outputs = model(images)[m[41m[m
[32m+[m[32m        initial_loss =  F.cross_entropy(outputs, labels)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        _, predicted = torch.max(outputs, 1)[m[41m [m
[32m+[m[41m        [m
[32m+[m[32m        initial_predictions.extend(predicted.cpu().numpy())[m[41m[m
[32m+[m[32m        initial_targets.extend(labels.cpu().numpy())[m[41m[m
[32m+[m[41m        [m
[32m+[m[32minitial_accuracy = 100 * (np.array(initial_predictions) == np.array(initial_targets)).sum() / len(initial_targets)[m[41m[m
[32m+[m[41m[m
[32m+[m[32mprint(f"First test, initial loss: {initial_loss.item():.20f}, initial accuracy: {initial_accuracy}")[m[41m[m
[32m+[m[32mwith open(results_dir + timestamp, "a") as f:[m[41m[m
[32m+[m[32m    f.write(f"First test, initial loss: {initial_loss.item():.20f}, initial accuracy: {initial_accuracy} \n")[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[32m+[m[32mnum_epochs = 100[m[41m[m
[32m+[m[41m[m
[32m+[m[32mfor epoch in range(num_epochs):[m[41m[m
[32m+[m[32m    total_correct = 0[m[41m[m
[32m+[m[32m    total_samples = 0[m[41m[m
[32m+[m[32m    total_test_correct = 0[m[41m[m
[32m+[m[32m    total_test_samples = 0[m[41m[m
[32m+[m[41m[m
[32m+[m[32m    train_predictions = [][m[41m[m
[32m+[m[32m    train_targets = [][m[41m[m
[32m+[m[32m    test_predictions = [][m[41m[m
[32m+[m[32m    test_targets = [][m[41m[m
[32m+[m[41m[m
[32m+[m[32m    correct_samples = [][m[41m[m
[32m+[m[32m    incorrect_samples = [][m[41m[m
[32m+[m[41m[m
[32m+[m[32m    model.train()[m[41m[m
[32m+[m[41m[m
[32m+[m[32m    for images, labels in train_loader:[m[41m[m
[32m+[m[32m        images, labels = images.to(device, non_blocking=True), labels.to([m[41m[m
[32m+[m[32m            device, non_blocking=True)[m[41m[m
[32m+[m[32m        # Forward pass[m[41m[m
[32m+[m[32m        with autocast():[m[41m[m
[32m+[m[32m            outputs = model(images)[m[41m[m
[32m+[m[32m        loss = F.cross_entropy(outputs, labels)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        # Backward pass[m[41m[m
[32m+[m[32m        optimizer.zero_grad()[m[41m[m
[32m+[m[32m        loss.backward()[m[41m[m
[32m+[m[32m        optimizer.step()[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        # Calculate accuracy[m[41m[m
[32m+[m[32m        _, predicted = torch.max(outputs, 1)[m[41m[m
[32m+[m[32m        total_correct += (predicted == labels).sum().item()[m[41m[m
[32m+[m[32m        total_samples += labels.size(0)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m    train_accuracy = 100 * total_correct / total_samples[m[41m[m
[32m+[m[32m    train_losses.append(loss.item())[m[41m[m
[32m+[m[32m    train_accuracies.append(train_accuracy)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m    model.eval()[m[41m[m
[32m+[m[32m    with torch.no_grad():[m[41m[m
[32m+[m[32m        for images, labels in test_loader:[m[41m[m
[32m+[m[32m            images = images.to(device)[m[41m[m
[32m+[m[32m            labels = labels.to(device)[m[41m[m
[32m+[m[32m            with autocast():[m[41m[m
[32m+[m[32m                outputs = model(images)[m[41m[m
[32m+[m[32m            test_loss = F.cross_entropy(outputs, labels)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m            _, predicted = torch.max(outputs, 1)[m[41m[m
[32m+[m[32m            total_test_correct += (predicted == labels).sum().item()[m[41m[m
[32m+[m[32m            total_test_samples += labels.size(0)[m[41m[m
[32m+[m[32m            test_predictions.extend(predicted.cpu().numpy())[m[41m[m
[32m+[m[32m            test_targets.extend(labels.cpu().numpy())[m[41m[m
[32m+[m[41m[m
[32m+[m[32m            matches = predicted == labels[m[41m[m
[32m+[m[32m            for i in range(images.size(0)):[m[41m[m
[32m+[m[32m                if matches[i]:[m[41m[m
[32m+[m[32m                    correct_samples.append([m[41m[m
[32m+[m[32m                        (images[i], labels[i], predicted[i]))[m[41m[m
[32m+[m[32m                elif not matches[i]:[m[41m[m
[32m+[m[32m                    incorrect_samples.append([m[41m[m
[32m+[m[32m                        (images[i], labels[i], predicted[i]))[m[41m[m
[32m+[m[41m[m
[32m+[m[32m    test_accuracy = 100 * total_test_correct / total_test_samples[m[41m[m
[32m+[m[32m    test_losses.append(test_loss.item())[m[41m[m
[32m+[m[32m    test_accuracies.append(test_accuracy)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m    scheduler.step(loss)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m    train_accuracy = 100 * total_correct / total_samples[m[41m[m
[32m+[m[32m    print(f"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {loss.item():.20f}, Train Accuracy: {train_accuracy:.2f}%, Test Loss: {test_loss.item():.20f}, Test Accuracy: {test_accuracy:.2f}%")[m[41m[m
[32m+[m[41m    [m
[32m+[m[32m    if (epoch + 1) % 10 == 0:[m[41m[m
[32m+[m[32m        torch.save(model.state_dict(), save_path.format(epoch + 1))[m[41m[m
[32m+[m[32m        print(f"Saved model at epoch {epoch + 1}")[m[41m[m
[32m+[m[32m        cm = confusion_matrix(test_targets, test_predictions)[m[41m[m
[32m+[m[32m        report = classification_report(test_targets, test_predictions, digits=3)[m[41m[m
[32m+[m[32m        print(f"Confusion Matrix after Epoch {epoch + 1}:\n{cm}")[m[41m[m
[32m+[m[32m        print(f"Classification Report after Epoch {epoch + 1}:\n{report}")[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        with open(results_dir + timestamp, "a") as f:[m[41m[m
[32m+[m[32m            f.write(f"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {loss.item():.20f}, Train Accuracy: {train_accuracy:.2f}%, Test Loss: {test_loss.item():.20f}, Test Accuracy: {test_accuracy:.2f}% \n")[m[41m[m
[32m+[m[32m            f.write(f"Confusion Matrix after Epoch {epoch + 1}:\n{cm} \n")[m[41m[m
[32m+[m[32m            f.write(f"Classification Report after Epoch {epoch + 1}:\n{report} \n")[m[41m[m
[32m+[m[41m[m
[32m+[m[32mprint(f"Number of wrong guesses in last training = {len(incorrect_samples)}")[m[41m[m
[32m+[m[32mprint(f"Number of right guesses in last training = {len(correct_samples)}")[m[41m[m
[32m+[m[41m[m
[32m+[m[32mplt.subplot(1, 2, 1)[m[41m[m
[32m+[m[32mplt.plot(train_losses, label="Train Loss")[m[41m[m
[32m+[m[32mplt.plot(test_losses, label="Test Loss")[m[41m[m
[32m+[m[32mplt.title("Loss over epochs")[m[41m[m
[32m+[m[32mplt.xlabel("Epoch")[m[41m[m
[32m+[m[32mplt.ylabel("Loss")[m[41m[m
[32m+[m[32mplt.legend()[m[41m[m
[32m+[m[41m[m
[32m+[m[32m# Plotting accuracy[m[41m[m
[32m+[m[32mplt.subplot(1, 2, 2)[m[41m[m
[32m+[m[32mplt.plot(train_accuracies, label="Train Accuracy")[m[41m[m
[32m+[m[32mplt.plot(test_accuracies, label="Test Accuracy")[m[41m[m
[32m+[m[32mplt.title("Accuracy over epochs")[m[41m[m
[32m+[m[32mplt.xlabel("Epoch")[m[41m[m
[32m+[m[32mplt.ylabel("Accuracy (%)")[m[41m[m
[32m+[m[32mplt.legend()[m[41m[m
[32m+[m[41m[m
[32m+[m[32mplot_samples(correct_samples[0:10], "Correctly Predicted Samples")[m[41m[m
[32m+[m[32mplot_samples(incorrect_samples[0:10], "Incorrectly Predicted Samples")[m[41m[m
[32m+[m[41m[m
[32m+[m[32mplt.tight_layout()[m[41m[m
[32m+[m[32mplt.show()[m[41m[m
[1mdiff --git a/Uppgift_2/CUB-200_pre-trained.py b/Uppgift_2/CUB-200_pre-trained.py[m
[1mnew file mode 100644[m
[1mindex 0000000..d0e6cee[m
[1m--- /dev/null[m
[1m+++ b/Uppgift_2/CUB-200_pre-trained.py[m
[36m@@ -0,0 +1,289 @@[m
[32m+[m[32mimport torch[m[41m[m
[32m+[m[32mimport torch.nn as nn[m[41m[m
[32m+[m[32mimport torchvision.models as models[m[41m[m
[32m+[m[32mimport torch.optim as optim[m[41m[m
[32m+[m[32mimport torch.nn.functional as F[m[41m[m
[32m+[m[32mfrom torch.utils.data import DataLoader[m[41m[m
[32m+[m[32mfrom torch.utils.data import Dataset[m[41m[m
[32m+[m[32mimport os[m[41m[m
[32m+[m[32mimport pandas as pd[m[41m[m
[32m+[m[32mfrom torchvision.io import read_image[m[41m[m
[32m+[m[32mfrom torchvision import transforms[m[41m[m
[32m+[m[32mfrom torch.cuda.amp import autocast, GradScaler[m[41m[m
[32m+[m[32mfrom torch.optim.lr_scheduler import ReduceLROnPlateau[m[41m[m
[32m+[m[32mimport matplotlib.pyplot as plt[m[41m[m
[32m+[m[32mimport numpy as np[m[41m[m
[32m+[m[32mimport time[m[41m[m
[32m+[m[32mimport os[m[41m[m
[32m+[m[32mfrom sklearn.metrics import confusion_matrix, classification_report[m[41m[m
[32m+[m[32mfrom pathlib import Path[m[41m[m
[32m+[m[32mfrom torchvision.transforms import v2[m[41m[m
[32m+[m[32mfrom torchvision.io import read_image[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[32m+[m[32mclass CUBDataset(Dataset):[m[41m[m
[32m+[m[32m    def __init__(self, data_dir, transform=None):[m[41m[m
[32m+[m[32m        self.data_dir = data_dir[m[41m[m
[32m+[m[32m        self.transform = transform[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        # Load image-to-class mapping with updated delimiter[m[41m[m
[32m+[m[32m        image_class_path = os.path.join(data_dir, 'images.txt')[m[41m[m
[32m+[m[32m        self.image_class_itterate = pd.read_csv([m[41m[m
[32m+[m[32m            image_class_path, sep=r"\s+", engine='python', header=None)[m[41m[m
[32m+[m[32m        # Construct image paths[m[41m[m
[32m+[m[32m        self.image_paths = [os.path.join(data_dir, 'images', f"{row[1]}") for _, row in self.image_class_itterate.iterrows()][m[41m[m
[32m+[m[32m        image_class_labels_path = os.path.join([m[41m[m
[32m+[m[32m            data_dir, 'image_class_labels.txt')[m[41m[m
[32m+[m[32m        self.image_class_labels = pd.read_csv([m[41m[m
[32m+[m[32m            image_class_labels_path, sep=r"\s+", engine='python', header=None)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m    def __len__(self):[m[41m[m
[32m+[m[32m        return len(self.image_class_labels)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m    def __getitem__(self, idx):[m[41m[m
[32m+[m[32m        image_path = self.image_paths[idx][m[41m[m
[32m+[m[41m[m
[32m+[m[32m        # Check if file exists to prevent "File not found" error[m[41m[m
[32m+[m[32m        if not os.path.exists(image_path):[m[41m[m
[32m+[m[32m            raise ValueError(f"File not found: {image_path}")[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        image = read_image(image_path)  # Read the image[m[41m[m
[32m+[m[32m        # Adjust for zero-based index[m[41m[m
[32m+[m[32m        label = int(self.image_class_labels.iloc[idx, 1]) - 1[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        if self.transform:[m[41m[m
[32m+[m[32m            image = self.transform(image)  # Apply transformation[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        return image, label[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[32m+[m[32mclass GrayscaleToRGB:[m[41m[m
[32m+[m[32m    def __call__(self, img):[m[41m[m
[32m+[m[32m        # If the image is grayscale, convert it to RGB by repeating the channel[m[41m[m
[32m+[m[32m        if img.shape[0] == 1:[m[41m[m
[32m+[m[32m            # Repeat the single channel to make it 3-channel (RGB)[m[41m[m
[32m+[m[32m            img = img.repeat(3, 1, 1)[m[41m[m
[32m+[m[32m        return img[m[41m[m
[32m+[m[41m    [m
[32m+[m[32mdef plot_samples(samples, title):[m[41m[m
[32m+[m[32m    plt.figure(figsize=(10, 5))[m[41m[m
[32m+[m[32m    for i, (image, label, pred) in enumerate(samples):[m[41m[m
[32m+[m[32m        image = image.cpu().numpy().transpose((1, 2, 0))  # rearrange for matplotlib[m[41m[m
[32m+[m[32m        mean = np.array([0.5])[m[41m[m
[32m+[m[32m        std = np.array([0.5])[m[41m[m
[32m+[m[32m        image = std * image + mean  # unnormalize[m[41m[m
[32m+[m[32m        image = np.clip(image, 0, 1)[m[41m[m
[32m+[m[32m        plt.subplot(2, 5, i + 1)[m[41m[m
[32m+[m[32m        plt.imshow(image[:, :, 0], cmap="gray")[m[41m[m
[32m+[m[32m        plt.title(f"Label: {label}\nPred: {pred}")[m[41m[m
[32m+[m[32m        plt.axis("off")[m[41m[m
[32m+[m[32m    plt.suptitle(title)[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[32m+[m[32mtrain_transform = transforms.Compose([[m[41m[m
[32m+[m[32m    GrayscaleToRGB(),  # Convert grayscale to RGB if needed[m[41m[m
[32m+[m[32m    transforms.ToPILImage(),[m[41m[m
[32m+[m[32m    transforms.Resize(256),[m[41m[m
[32m+[m[32m    transforms.CenterCrop(224),[m[41m[m
[32m+[m[32m    transforms.RandomHorizontalFlip(),[m[41m[m
[32m+[m[32m    transforms.RandomRotation(10),[m[41m[m
[32m+[m[32m    transforms.ToTensor(),[m[41m[m
[32m+[m[32m    transforms.Normalize([0.485, 0.456, 0.406], [[m[41m[m
[32m+[m[32m                         0.229, 0.224, 0.225])  # Normalization for RGB[m[41m[m
[32m+[m[32m])[m[41m[m
[32m+[m[41m[m
[32m+[m[32mtest_transform = transforms.Compose([[m[41m[m
[32m+[m[32m    GrayscaleToRGB(),  # Ensure RGB consistency[m[41m[m
[32m+[m[32m    transforms.ToPILImage(),[m[41m[m
[32m+[m[32m    transforms.Resize(256),[m[41m[m
[32m+[m[32m    transforms.CenterCrop(224),[m[41m[m
[32m+[m[32m    transforms.ToTensor(),[m[41m[m
[32m+[m[32m    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])[m[41m[m
[32m+[m[32m])[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[32m+[m[32m# Load a pre-trained ResNet model[m[41m[m
[32m+[m[32mmodel = models.resnet50(weights="ResNet50_Weights.IMAGENET1K_V1")[m[41m[m
[32m+[m[41m[m
[32m+[m[32m# Modify the output layer to match your bird dataset's class count[m[41m[m
[32m+[m[32mnum_classes = 200  # Change to your specific bird class count[m[41m[m
[32m+[m[32mmodel.fc = nn.Linear(model.fc.in_features, num_classes)[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[32m+[m[32mdata_dir = "./data/CUB-200/CUB_200_2011/"[m[41m[m
[32m+[m[32mdevice = ("cuda")[m[41m[m
[32m+[m[41m[m
[32m+[m[32msave_dir = "./models/CUB-200_pretrained"[m[41m[m
[32m+[m[32mif not os.path.exists(save_dir):[m[41m[m
[32m+[m[32m    os.makedirs(save_dir)[m[41m[m
[32m+[m[41m[m
[32m+[m[32mtimestamp = time.strftime("%Y%m%d-%H%M%S")[m[41m[m
[32m+[m[41m[m
[32m+[m[32mweight_dir = f"{save_dir}/model_{timestamp}/weights"[m[41m[m
[32m+[m[32mresults_dir = f"{save_dir}/model_{timestamp}/results"[m[41m[m
[32m+[m[41m[m
[32m+[m[32mif not os.path.exists(weight_dir):[m[41m[m
[32m+[m[32m    os.makedirs(weight_dir)[m[41m[m
[32m+[m[41m[m
[32m+[m[32msave_path = weight_dir+"/epoch_{}.pth"[m[41m[m
[32m+[m[41m[m
[32m+[m[32m# Create custom datasets for training and testing[m[41m[m
[32m+[m[32mtrain_dataset = CUBDataset(data_dir, transform=train_transform)[m[41m[m
[32m+[m[32mtest_dataset = CUBDataset(data_dir, transform=test_transform)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m# Create DataLoaders for batching and shuffling[m[41m[m
[32m+[m[32mtrain_loader = DataLoader(train_dataset, batch_size=64,[m[41m[m
[32m+[m[32m                          shuffle=True, pin_memory=True)[m[41m[m
[32m+[m[32mtest_loader = DataLoader(test_dataset, batch_size=64,[m[41m[m
[32m+[m[32m                         shuffle=False, pin_memory=True)[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[32m+[m[32minput_shape = (3, 224, 224)[m[41m[m
[32m+[m[32moutput_size = 200[m[41m [m
[32m+[m[32mmodel = model.to(device)[m[41m[m
[32m+[m[41m[m
[32m+[m[32moptimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)[m[41m[m
[32m+[m[32mscaler = GradScaler()[m[41m[m
[32m+[m[32mscheduler = ReduceLROnPlateau(optimizer, mode="min", factor=0.2, patience=10)[m[41m[m
[32m+[m[41m[m
[32m+[m[32mtrain_accuracies = [][m[41m[m
[32m+[m[32mtest_accuracies = [][m[41m[m
[32m+[m[32mtrain_losses = [][m[41m[m
[32m+[m[32mtest_losses = [][m[41m[m
[32m+[m[41m[m
[32m+[m[32mmodel.eval()[m[41m[m
[32m+[m[32minitial_predictions = [][m[41m[m
[32m+[m[32minitial_targets = [][m[41m[m
[32m+[m[41m[m
[32m+[m[32mwith torch.no_grad(): #no training for first test[m[41m[m
[32m+[m[32m    for images, labels in test_loader:[m[41m[m
[32m+[m[32m        images, labels = images.to(device), labels.to(device)[m[41m[m
[32m+[m[41m        [m
[32m+[m[32m        outputs = model(images)[m[41m[m
[32m+[m[32m        initial_loss =  F.cross_entropy(outputs, labels)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        _, predicted = torch.max(outputs, 1)[m[41m [m
[32m+[m[41m        [m
[32m+[m[32m        initial_predictions.extend(predicted.cpu().numpy())[m[41m[m
[32m+[m[32m        initial_targets.extend(labels.cpu().numpy())[m[41m[m
[32m+[m[41m[m
[32m+[m[32minitial_accuracy = 100 * (np.array(initial_predictions) == np.array(initial_targets)).sum() / len(initial_targets)[m[41m[m
[32m+[m[41m[m
[32m+[m[32mprint(f"First test, initial loss: {initial_loss.item():.20f}, initial accuracy: {initial_accuracy}")[m[41m[m
[32m+[m[32mwith open(results_dir + timestamp, "a") as f:[m[41m[m
[32m+[m[32m    f.write(f"First test, initial loss: {initial_loss.item():.20f}, initial accuracy: {initial_accuracy} \n")[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[32m+[m[32mnum_epochs = 100[m[41m[m
[32m+[m[41m[m
[32m+[m[32mfor epoch in range(num_epochs):[m[41m[m
[32m+[m[32m    total_correct = 0[m[41m[m
[32m+[m[32m    total_samples = 0[m[41m[m
[32m+[m[32m    total_test_correct = 0[m[41m[m
[32m+[m[32m    total_test_samples = 0[m[41m[m
[32m+[m[41m[m
[32m+[m[32m    train_predictions = [][m[41m[m
[32m+[m[32m    train_targets = [][m[41m[m
[32m+[m[32m    test_predictions = [][m[41m[m
[32m+[m[32m    test_targets = [][m[41m[m
[32m+[m[41m[m
[32m+[m[32m    correct_samples = [][m[41m[m
[32m+[m[32m    incorrect_samples = [][m[41m[m
[32m+[m[41m[m
[32m+[m[32m    model.train()[m[41m[m
[32m+[m[41m[m
[32m+[m[32m    for images, labels in train_loader:[m[41m[m
[32m+[m[32m        images, labels = images.to(device, non_blocking=True), labels.to([m[41m[m
[32m+[m[32m            device, non_blocking=True)[m[41m[m
[32m+[m[32m        # Forward pass[m[41m[m
[32m+[m[32m        with autocast():[m[41m[m
[32m+[m[32m            outputs = model(images)[m[41m[m
[32m+[m[32m        loss = F.cross_entropy(outputs, labels)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        # Backward pass[m[41m[m
[32m+[m[32m        optimizer.zero_grad()[m[41m[m
[32m+[m[32m        loss.backward()[m[41m[m
[32m+[m[32m        optimizer.step()[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        # Calculate accuracy[m[41m[m
[32m+[m[32m        _, predicted = torch.max(outputs, 1)[m[41m[m
[32m+[m[32m        total_correct += (predicted == labels).sum().item()[m[41m[m
[32m+[m[32m        total_samples += labels.size(0)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m    train_accuracy = 100 * total_correct / total_samples[m[41m[m
[32m+[m[32m    train_losses.append(loss.item())[m[41m[m
[32m+[m[32m    train_accuracies.append(train_accuracy)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m    model.eval()[m[41m[m
[32m+[m[32m    with torch.no_grad():[m[41m[m
[32m+[m[32m        for images, labels in test_loader:[m[41m[m
[32m+[m[32m            images = images.to(device)[m[41m[m
[32m+[m[32m            labels = labels.to(device)[m[41m[m
[32m+[m[32m            with autocast():[m[41m[m
[32m+[m[32m                outputs = model(images)[m[41m[m
[32m+[m[32m            test_loss = F.cross_entropy(outputs, labels)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m            _, predicted = torch.max(outputs, 1)[m[41m[m
[32m+[m[32m            total_test_correct += (predicted == labels).sum().item()[m[41m[m
[32m+[m[32m            total_test_samples += labels.size(0)[m[41m[m
[32m+[m[32m            test_predictions.extend(predicted.cpu().numpy())[m[41m[m
[32m+[m[32m            test_targets.extend(labels.cpu().numpy())[m[41m[m
[32m+[m[41m[m
[32m+[m[32m            matches = predicted == labels[m[41m[m
[32m+[m[32m            for i in range(images.size(0)):[m[41m[m
[32m+[m[32m                if matches[i]:[m[41m[m
[32m+[m[32m                    correct_samples.append([m[41m[m
[32m+[m[32m                        (images[i], labels[i], predicted[i]))[m[41m[m
[32m+[m[32m                elif not matches[i]:[m[41m[m
[32m+[m[32m                    incorrect_samples.append([m[41m[m
[32m+[m[32m                        (images[i], labels[i], predicted[i]))[m[41m[m
[32m+[m[41m[m
[32m+[m[32m    test_accuracy = 100 * total_test_correct / total_test_samples[m[41m[m
[32m+[m[32m    test_losses.append(test_loss.item())[m[41m[m
[32m+[m[32m    test_accuracies.append(test_accuracy)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m    scheduler.step(loss)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m    train_accuracy = 100 * total_correct / total_samples[m[41m[m
[32m+[m[32m    print(f"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {loss.item():.20f}, Train Accuracy: {train_accuracy:.2f}%, Test Loss: {test_loss.item():.20f}, Test Accuracy: {test_accuracy:.2f}%")[m[41m[m
[32m+[m[41m    [m
[32m+[m[32m    if (epoch + 1) % 10 == 0:[m[41m[m
[32m+[m[32m        torch.save(model.state_dict(), save_path.format(epoch + 1))[m[41m[m
[32m+[m[32m        print(f"Saved model at epoch {epoch + 1}")[m[41m[m
[32m+[m[32m        cm = confusion_matrix(test_targets, test_predictions)[m[41m[m
[32m+[m[32m        report = classification_report(test_targets, test_predictions, digits=3)[m[41m[m
[32m+[m[32m        print(f"Confusion Matrix after Epoch {epoch + 1}:\n{cm}")[m[41m[m
[32m+[m[32m        print(f"Classification Report after Epoch {epoch + 1}:\n{report}")[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        with open(results_dir + timestamp, "a") as f:[m[41m[m
[32m+[m[32m            f.write(f"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {loss.item():.20f}, Train Accuracy: {train_accuracy:.2f}%, Test Loss: {test_loss.item():.20f}, Test Accuracy: {test_accuracy:.2f}% \n")[m[41m[m
[32m+[m[32m            f.write(f"Confusion Matrix after Epoch {epoch + 1}:\n{cm} \n")[m[41m[m
[32m+[m[32m            f.write(f"Classification Report after Epoch {epoch + 1}:\n{report} \n")[m[41m[m
[32m+[m[41m[m
[32m+[m[32mprint(f"Number of wrong guesses in last training = {len(incorrect_samples)}")[m[41m[m
[32m+[m[32mprint(f"Number of right guesses in last training = {len(correct_samples)}")[m[41m[m
[32m+[m[41m[m
[32m+[m[32mplt.subplot(1, 2, 1)[m[41m[m
[32m+[m[32mplt.plot(train_losses, label="Train Loss")[m[41m[m
[32m+[m[32mplt.plot(test_losses, label="Test Loss")[m[41m[m
[32m+[m[32mplt.title("Loss over epochs")[m[41m[m
[32m+[m[32mplt.xlabel("Epoch")[m[41m[m
[32m+[m[32mplt.ylabel("Loss")[m[41m[m
[32m+[m[32mplt.legend()[m[41m[m
[32m+[m[41m[m
[32m+[m[32m# Plotting accuracy[m[41m[m
[32m+[m[32mplt.subplot(1, 2, 2)[m[41m[m
[32m+[m[32mplt.plot(train_accuracies, label="Train Accuracy")[m[41m[m
[32m+[m[32mplt.plot(test_accuracies, label="Test Accuracy")[m[41m[m
[32m+[m[32mplt.title("Accuracy over epochs")[m[41m[m
[32m+[m[32mplt.xlabel("Epoch")[m[41m[m
[32m+[m[32mplt.ylabel("Accuracy (%)")[m[41m[m
[32m+[m[32mplt.legend()[m[41m[m
[32m+[m[41m[m
[32m+[m[32mplot_samples(correct_samples[0:10], "Correctly Predicted Samples")[m[41m[m
[32m+[m[32mplot_samples(incorrect_samples[0:10], "Incorrectly Predicted Samples")[m[41m[m
[32m+[m[41m[m
[32m+[m[32mplt.tight_layout()[m[41m[m
[32m+[m[32mplt.show()[m[41m[m
[32m+[m[41m[m
[1mdiff --git a/Uppgift_2/D.py b/Uppgift_2/D.py[m
[1mnew file mode 100644[m
[1mindex 0000000..9d2e628[m
[1m--- /dev/null[m
[1m+++ b/Uppgift_2/D.py[m
[36m@@ -0,0 +1,261 @@[m
[32m+[m[32mimport torch[m[41m[m
[32m+[m[32mimport torch.nn as nn[m[41m[m
[32m+[m[32mimport torch.optim as optim[m[41m[m
[32m+[m[32mfrom torchvision import datasets, transforms[m[41m[m
[32m+[m[32mfrom torch.utils.data import DataLoader[m[41m[m
[32m+[m[32mfrom torch.cuda.amp import autocast, GradScaler[m[41m[m
[32m+[m[32mfrom torch.optim.lr_scheduler import ReduceLROnPlateau[m[41m[m
[32m+[m[32mimport matplotlib.pyplot as plt[m[41m[m
[32m+[m[32mimport numpy as np[m[41m[m
[32m+[m[32mimport time[m[41m[m
[32m+[m[32mimport os[m[41m[m
[32m+[m[32mfrom sklearn.metrics import confusion_matrix, classification_report[m[41m[m
[32m+[m[32mfrom pathlib import Path[m[41m[m
[32m+[m[32mfrom torchvision.transforms import v2[m[41m[m
[32m+[m[32mfrom torchvision.io import read_image[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[32m+[m[32mclass Perceptron(nn.Module):[m[41m[m
[32m+[m[32m    def __init__(self):[m[41m[m
[32m+[m[32m        super(Perceptron, self).__init__()[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        # convolutional layers[m[41m[m
[32m+[m[32m        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)[m[41m [m
[32m+[m[32m        self.bn1 = nn.BatchNorm2d(32)  # batch normalization after conv1[m[41m[m
[32m+[m[32m        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)[m[41m[m
[32m+[m[32m        self.bn2 = nn.BatchNorm2d(64)  # batch normalization after conv2[m[41m[m
[32m+[m[32m        self.pool = nn.MaxPool2d(2, 2)[m[41m[m
[32m+[m[32m        self.dropout = nn.Dropout(0.25)  #reduce overfitting[m[41m[m
[32m+[m[41m        [m
[32m+[m[32m        # fully connected layers[m[41m[m
[32m+[m[32m        self.flatten = nn.Flatten()  # flatten it for fully connected layers[m[41m[m
[32m+[m[32m        self.fc1 = nn.Linear(64 * 7 * 7, 128)[m[41m[m
[32m+[m[32m        self.fc2 = nn.Linear(128, 10)[m[41m[m
[32m+[m[41m        [m
[32m+[m[32m    def forward(self, x):[m[41m[m
[32m+[m[32m        #convulusional[m[41m[m
[32m+[m[32m        x = self.pool(nn.ReLU()(self.bn1(self.conv1(x))))[m[41m[m
[32m+[m[32m        x = self.pool(nn.ReLU()(self.bn2(self.conv2(x))))[m[41m[m
[32m+[m[41m        [m
[32m+[m[32m        #dropout[m[41m[m
[32m+[m[32m        x = self.dropout(x)[m[41m[m
[32m+[m[41m        [m
[32m+[m[32m        #fully connected layers[m[41m[m
[32m+[m[32m        x = self.flatten(x)[m[41m[m
[32m+[m[32m        x = nn.ReLU()(self.fc1(x))[m[41m[m
[32m+[m[32m        x = self.fc2(x)[m[41m[m
[32m+[m[41m        [m
[32m+[m[32m        return x[m[41m[m
[32m+[m[41m    [m
[32m+[m[32mclass AddGaussianNoise(object):[m[41m[m
[32m+[m[32m    def __init__(self, mean=0.0, std=0.1):[m[41m[m
[32m+[m[32m        self.mean = mean[m[41m[m
[32m+[m[32m        self.std = std[m[41m[m
[32m+[m[41m[m
[32m+[m[32m    def __call__(self, tensor):[m[41m[m
[32m+[m[32m        # Ensure that the tensor is a PyTorch tensor[m[41m[m
[32m+[m[32m        if not isinstance(tensor, torch.Tensor):[m[41m[m
[32m+[m[32m            raise ValueError("Input must be a PyTorch tensor")[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        # Add Gaussian noise with the correct size[m[41m[m
[32m+[m[32m        noise = torch.randn_like(tensor) * self.std + self.mean[m[41m[m
[32m+[m[32m        return tensor + noise[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[32m+[m[32mdef plot_samples(samples, title):[m[41m[m
[32m+[m[32m    plt.figure(figsize=(10, 5))[m[41m[m
[32m+[m[32m    for i, (image, label, pred) in enumerate(samples):[m[41m[m
[32m+[m[32m        image = image.cpu().numpy().transpose((1, 2, 0))  #rearrange for matplotlib[m[41m[m
[32m+[m[32m        mean = np.array([0.5])[m[41m[m
[32m+[m[32m        std = np.array([0.5])[m[41m[m
[32m+[m[32m        image = std * image + mean  #unnormalize[m[41m[m
[32m+[m[32m        image = np.clip(image, 0, 1)[m[41m[m
[32m+[m[32m        plt.subplot(2, 5, i + 1)[m[41m[m
[32m+[m[32m        plt.imshow(image[:, :, 0], cmap="gray")[m[41m[m
[32m+[m[32m        plt.title(f"Label: {label}\nPred: {pred}")[m[41m[m
[32m+[m[32m        plt.axis("off")[m[41m[m
[32m+[m[32m    plt.suptitle(title)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m#make cuda for gpu use[m[41m[m
[32m+[m[32mdevice = "cuda" if torch.cuda.is_available() else "cpu"[m[41m[m
[32m+[m[41m[m
[32m+[m[32m#transformer[m[41m[m
[32m+[m[32mtrain_transform = transforms.Compose([[m[41m[m
[32m+[m[32m    transforms.RandomRotation(10),  # rotate random 10 degrees[m[41m[m
[32m+[m[32m    transforms.ToTensor(),[m[41m [m
[32m+[m[32m    transforms.Normalize((0.5,), (0.5,))[m[41m[m
[32m+[m[32m])[m[41m[m
[32m+[m[41m[m
[32m+[m[32mtest_transform = transforms.Compose([[m[41m[m
[32m+[m[32m    transforms.ToTensor(),[m[41m[m
[32m+[m[32m    transforms.Normalize((0.5,), (0.5,))  # for zero mean and unit variance[m[41m[m
[32m+[m[32m])[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[32m+[m[32mtrain_set = datasets.MNIST(root="./data", train=True, download=True, transform=train_transform)[m[41m[m
[32m+[m[32mtest_set = datasets.MNIST(root="./data", train=False, download=True, transform=test_transform)[m[41m[m
[32m+[m[41m[m
[32m+[m[32mtrain_loader = DataLoader(train_set, batch_size=64, shuffle=True, pin_memory=True)[m[41m[m
[32m+[m[32mtest_loader = DataLoader(test_set, batch_size=64, shuffle=False, pin_memory=True)[m[41m[m
[32m+[m[41m[m
[32m+[m[32mmodel = Perceptron().to(device)[m[41m[m
[32m+[m[41m[m
[32m+[m[32mcriterion = nn.CrossEntropyLoss()[m[41m  [m
[32m+[m[32moptimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)[m[41m  [m
[32m+[m[41m[m
[32m+[m[32mscaler = GradScaler()[m[41m[m
[32m+[m[32mscheduler = ReduceLROnPlateau(optimizer, mode="min", factor=0.2, patience=10)[m[41m[m
[32m+[m[32mtrain_accuracies = [][m[41m[m
[32m+[m[32mtest_accuracies = [][m[41m[m
[32m+[m[32mtrain_losses = [][m[41m[m
[32m+[m[32mtest_losses = [][m[41m[m
[32m+[m[32mepsilon = 1e-10[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[32m+[m[32mtimestamp = time.strftime("%Y%m%d-%H%M%S")[m[41m[m
[32m+[m[41m[m
[32m+[m[32msave_dir = "./models/D"[m[41m[m
[32m+[m[32mif not os.path.exists(save_dir):[m[41m[m
[32m+[m[32m    os.makedirs(save_dir)[m[41m[m
[32m+[m[41m[m
[32m+[m[32mweight_dir = f"{save_dir}/model_{timestamp}/weights"[m[41m[m
[32m+[m[32mresults_dir = f"{save_dir}/model_{timestamp}/results"[m[41m[m
[32m+[m[41m[m
[32m+[m[32mif not os.path.exists(weight_dir):[m[41m[m
[32m+[m[32m    os.makedirs(weight_dir)[m[41m[m
[32m+[m[41m[m
[32m+[m[32msave_path = weight_dir+"/epoch_{}.pth"[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[32m+[m[32mmodel.eval()[m[41m[m
[32m+[m[32minitial_predictions = [][m[41m[m
[32m+[m[32minitial_targets = [][m[41m[m
[32m+[m[41m[m
[32m+[m[32mwith torch.no_grad(): #no training for first test[m[41m[m
[32m+[m[32m    for images, labels in test_loader:[m[41m[m
[32m+[m[32m        images, labels = images.to(device), labels.to(device)[m[41m[m
[32m+[m[41m        [m
[32m+[m[32m        outputs = model(images)[m[41m[m
[32m+[m[32m        initial_loss = criterion(outputs + epsilon, labels) * labels.size(0)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        _, predicted = torch.max(outputs, 1)[m[41m [m
[32m+[m[41m        [m
[32m+[m[32m        initial_predictions.extend(predicted.cpu().numpy())[m[41m[m
[32m+[m[32m        initial_targets.extend(labels.cpu().numpy())[m[41m[m
[32m+[m[41m        [m
[32m+[m[32minitial_accuracy = 100 * (np.array(initial_predictions) == np.array(initial_targets)).sum() / len(initial_targets)[m[41m[m
[32m+[m[41m[m
[32m+[m[32mprint(f"First test, initial loss: {initial_loss.item():.20f}, initial accuracy: {initial_accuracy}")[m[41m[m
[32m+[m[32mwith open(results_dir + timestamp, "a") as f:[m[41m[m
[32m+[m[32m    f.write(f"First test, initial loss: {initial_loss.item():.20f}, initial accuracy: {initial_accuracy}")[m[41m[m
[32m+[m[41m[m
[32m+[m[32mnum_epochs = 50[m[41m[m
[32m+[m[32mfor epoch in range(num_epochs):[m[41m[m
[32m+[m[32m    total_correct = 0[m[41m[m
[32m+[m[32m    total_samples = 0[m[41m[m
[32m+[m[32m    total_test_correct = 0[m[41m[m
[32m+[m[32m    total_test_samples = 0[m[41m[m
[32m+[m[41m[m
[32m+[m[32m    train_predictions = [][m[41m[m
[32m+[m[32m    train_targets = [][m[41m[m
[32m+[m[32m    test_predictions = [][m[41m[m
[32m+[m[32m    test_targets = [][m[41m[m
[32m+[m[41m[m
[32m+[m[32m    correct_samples = [][m[41m[m
[32m+[m[32m    incorrect_samples = [][m[41m[m
[32m+[m[41m[m
[32m+[m[32m    model.train()[m[41m[m
[32m+[m[32m    for images, labels in train_loader:[m[41m[m
[32m+[m[32m        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        optimizer.zero_grad()[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        with autocast():[m[41m[m
[32m+[m[32m            outputs = model(images)[m[41m[m
[32m+[m[32m            loss = criterion(outputs + epsilon, labels) * labels.size(0)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        scaler.scale(loss).backward()[m[41m[m
[32m+[m[32m        scaler.step(optimizer)[m[41m[m
[32m+[m[32m        scaler.update()[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        _, predicted = torch.max(outputs, 1)[m[41m  [m
[32m+[m[32m        total_correct += (predicted == labels).sum().item()[m[41m[m
[32m+[m[32m        total_samples += labels.size(0)[m[41m[m
[32m+[m[32m        train_predictions.extend(predicted.cpu().numpy())[m[41m  [m
[32m+[m[32m        train_targets.extend(labels.cpu().numpy())[m[41m  [m
[32m+[m[41m[m
[32m+[m[32m    train_accuracy = 100 * total_correct / total_samples[m[41m[m
[32m+[m[32m    train_losses.append(loss.item())[m[41m[m
[32m+[m[32m    train_accuracies.append(train_accuracy)[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[32m+[m[32m    model.eval()[m[41m[m
[32m+[m[32m    with torch.no_grad():[m[41m[m
[32m+[m[32m        for images, labels in test_loader:[m[41m[m
[32m+[m[32m            images = images.to(device)[m[41m[m
[32m+[m[32m            labels= labels.to(device)[m[41m[m
[32m+[m[32m            with autocast():[m[41m[m
[32m+[m[32m                outputs = model(images)[m[41m[m
[32m+[m[32m            test_loss = criterion(outputs + epsilon, labels) * labels.size(0)[m[41m[m
[32m+[m[41m            [m
[32m+[m[32m            _, predicted = torch.max(outputs, 1)[m[41m[m
[32m+[m[32m            total_test_correct += (predicted == labels).sum().item()[m[41m[m
[32m+[m[32m            total_test_samples += labels.size(0)[m[41m[m
[32m+[m[32m            test_predictions.extend(predicted.cpu().numpy())[m[41m[m
[32m+[m[32m            test_targets.extend(labels.cpu().numpy())[m[41m[m
[32m+[m[41m[m
[32m+[m[32m            matches = predicted == labels[m[41m[m
[32m+[m[32m            for i in range(images.size(0)):[m[41m[m
[32m+[m[32m                if matches[i]:[m[41m [m
[32m+[m[32m                    correct_samples.append((images[i], labels[i], predicted[i]))[m[41m[m
[32m+[m[32m                elif not matches[i]:[m[41m[m
[32m+[m[32m                    incorrect_samples.append((images[i], labels[i], predicted[i]))[m[41m[m
[32m+[m[41m[m
[32m+[m[32m    test_accuracy = 100 * total_test_correct / total_test_samples[m[41m[m
[32m+[m[32m    test_losses.append(test_loss.item())[m[41m[m
[32m+[m[32m    test